# Research Methods Knowledge Base

## Overview

This comprehensive knowledge base provides detailed guidance on research methods for user research practitioners. Each method includes practical implementation details, best practices, and decision criteria.

## Table of Contents

1. [Qualitative Methods](#qualitative-methods)
2. [Quantitative Methods](#quantitative-methods)
3. [Mixed Methods](#mixed-methods)
4. [Method Selection Framework](#method-selection-framework)
5. [Sample Size Guidelines](#sample-size-guidelines)
6. [Timeline Estimations](#timeline-estimations)
7. [Cost Considerations](#cost-considerations)
8. [Quality Standards](#quality-standards)

---

## Qualitative Methods

### In-Depth Interviews (IDIs)

**Overview:** One-on-one conversations to explore individual experiences, motivations, and perspectives in detail.

**When to Use:**
- Exploring sensitive or personal topics
- Understanding individual decision-making processes
- Mapping detailed user journeys
- Investigating complex workflows
- Early exploratory research

**Key Characteristics:**
- Duration: 45-90 minutes typically
- Sample size: 15-30 participants (saturation usually at 12-15)
- Format: Structured, semi-structured, or unstructured
- Setting: In-person, video call, or phone

**Best Practices:**
1. Build rapport in first 5-10 minutes
2. Use open-ended questions
3. Practice active listening
4. Follow interesting threads while maintaining structure
5. Avoid leading questions
6. Record with permission
7. Take observation notes beyond just words

**Deliverables:**
- Interview transcripts
- Thematic analysis report
- Key quotes and insights
- Persona inputs
- Journey map inputs

**Common Pitfalls:**
- Interviewer bias influencing responses
- Not probing deep enough
- Over-structuring limiting discovery
- Poor participant screening

---

### Focus Groups

**Overview:** Facilitated group discussions to explore collective perspectives, generate ideas, and observe social dynamics.

**When to Use:**
- Concept testing and ideation
- Understanding social influences
- Exploring controversial topics
- Generating diverse perspectives quickly
- Brand perception research

**Key Characteristics:**
- Duration: 90-120 minutes
- Group size: 6-10 participants
- Number of groups: 3-5 typically
- Format: Moderated discussion with activities

**Best Practices:**
1. Create psychological safety for all participants
2. Manage dominant personalities
3. Encourage quiet participants
4. Use activities to engage (card sorting, voting)
5. Avoid groupthink through devil's advocate
6. Have assistant for note-taking
7. Set ground rules upfront

**Deliverables:**
- Discussion transcripts
- Theme synthesis across groups
- Areas of consensus/disagreement
- Idea prioritization
- Video highlights reel

**Common Pitfalls:**
- Groupthink limiting individual expression
- Logistics complexity
- One person dominating
- Superficial insights without depth

---

### Ethnographic Observation

**Overview:** Observing users in their natural environment to understand contextual behavior and unspoken needs.

**When to Use:**
- Understanding context of use
- Identifying unmet/unarticulated needs
- Studying complex workflows
- Cultural research
- Service design research

**Key Characteristics:**
- Duration: 2-8 hours per session
- Sample size: 8-15 participants
- Format: Passive observation, shadowing, or participatory
- Setting: User's natural environment

**Best Practices:**
1. Minimize observer influence
2. Document environment details
3. Note workarounds and pain points
4. Capture photos/videos when permitted
5. Conduct brief clarification interviews
6. Use observation protocol/checklist
7. Triangulate observations with interviews

**Deliverables:**
- Observation notes and photos
- Behavioral patterns analysis
- Context maps
- Opportunity areas
- Service blueprints

**Common Pitfalls:**
- Observer effect changing behavior
- Missing important details
- Over-interpreting observations
- Access and permission challenges

---

### Diary Studies

**Overview:** Participants self-document experiences over time to capture in-the-moment insights and longitudinal patterns.

**When to Use:**
- Understanding behavior over time
- Capturing in-the-moment experiences
- Studying infrequent events
- Mobile or distributed contexts
- Habit formation research

**Key Characteristics:**
- Duration: 1-4 weeks typically
- Sample size: 12-20 participants
- Format: Written, photo, video, or audio entries
- Frequency: Daily or event-triggered

**Best Practices:**
1. Provide clear instructions and examples
2. Send regular reminders
3. Keep entries brief (5-10 minutes)
4. Combine with interviews for depth
5. Provide easy capture tools
6. Offer incremental incentives
7. Check in weekly for engagement

**Deliverables:**
- Diary entries compilation
- Temporal pattern analysis
- Journey maps over time
- Moment maps
- Behavioral insights

**Common Pitfalls:**
- Participant drop-off over time
- Incomplete or superficial entries
- Recall bias in retrospective entries
- High participant burden

---

### Card Sorting

**Overview:** Participants organize information into categories to inform information architecture and mental models.

**When to Use:**
- Information architecture design
- Menu structure optimization
- Content categorization
- Understanding mental models
- Feature prioritization

**Key Characteristics:**
- Duration: 30-60 minutes
- Sample size: 15-30 participants
- Format: Open, closed, or hybrid sorting
- Setting: In-person or online tools

**Best Practices:**
1. Keep card count manageable (30-60)
2. Use clear, concise card labels
3. Allow "don't know" pile
4. Capture category names in open sorts
5. Discuss reasoning after sorting
6. Run pilot to refine cards
7. Combine with tree testing

**Deliverables:**
- Similarity matrix
- Dendrogram analysis
- Suggested IA structure
- Category labels
- Mental model insights

---

## Quantitative Methods

### Surveys

**Overview:** Structured questionnaires to collect standardized data from larger samples for statistical analysis.

**When to Use:**
- Measuring attitudes and preferences
- Validating qualitative findings
- Segmentation analysis
- Tracking metrics over time
- Market sizing

**Key Characteristics:**
- Duration: 5-20 minutes per response
- Sample size: 100-1000+ respondents
- Format: Online, phone, or in-person
- Question types: Closed-ended primarily

**Best Practices:**
1. Keep surveys focused and short
2. Pre-test questions for clarity
3. Randomize response options
4. Include attention checks
5. Avoid double-barreled questions
6. Use established scales when possible
7. Plan analysis before fielding

**Statistical Considerations:**
- Confidence level: 95% standard
- Margin of error: ±5% typical
- Sample size calculation: n = (Z²×p×q)/e²
- Response rate: Plan for 10-30% typically

**Deliverables:**
- Descriptive statistics
- Cross-tabulation analysis
- Segmentation analysis
- Statistical testing results
- Data visualizations

**Common Pitfalls:**
- Sampling bias
- Leading questions
- Survey fatigue
- Low response rates
- Misinterpreted statistics

---

### Conjoint Analysis

**Overview:** Statistical technique to understand how users value different attributes of a product or service.

**When to Use:**
- Feature prioritization
- Pricing optimization
- Product configuration
- Trade-off analysis
- Market simulation

**Key Characteristics:**
- Duration: 10-15 minutes per response
- Sample size: 200-500 respondents
- Format: Choice-based, ranking, or rating
- Attributes: 4-7 typically, 2-5 levels each

**Types:**
1. **Choice-Based Conjoint (CBC):** Most realistic, choose from sets
2. **Adaptive Conjoint (ACA):** Customizes to respondent
3. **Max-Diff:** Best-worst scaling
4. **Menu-Based:** Build your own product

**Best Practices:**
1. Limit attributes to avoid complexity
2. Use realistic attribute levels
3. Include "none" option in CBC
4. Balance attribute level presentation
5. Pilot test extensively
6. Use specialized software
7. Validate with holdout tasks

**Deliverables:**
- Importance scores
- Part-worth utilities
- Market simulation
- Willingness to pay
- Optimal product configuration

---

### A/B Testing

**Overview:** Controlled experiments comparing two or more variations to determine which performs better.

**When to Use:**
- Optimization decisions
- Validating design changes
- Measuring causal impact
- Risk mitigation before full rollout
- Settling debates with data

**Key Characteristics:**
- Duration: 1-4 weeks typically
- Sample size: Based on statistical power
- Format: Random assignment to variants
- Metrics: Primary and guardrail metrics

**Statistical Requirements:**
- Statistical power: 80% minimum
- Significance level: 5% (p < 0.05)
- Effect size: Minimum detectable effect
- Sample size: Use power analysis

**Best Practices:**
1. Define success metrics upfront
2. Run power analysis for sample size
3. Ensure random assignment
4. Check for sample ratio mismatch
5. Monitor guardrail metrics
6. Run for full business cycles
7. Account for multiple comparisons

**Deliverables:**
- Conversion rate comparison
- Statistical significance results
- Effect size and confidence intervals
- Recommendation with rationale
- Segment analysis

---

### Analytics Mining

**Overview:** Analyzing existing behavioral data to understand actual user behavior and identify patterns.

**When to Use:**
- Understanding current behavior
- Identifying drop-off points
- Segmentation analysis
- Baseline metrics establishment
- Hypothesis generation

**Data Sources:**
- Web/app analytics
- CRM data
- Support tickets
- Transaction logs
- Search queries

**Analysis Techniques:**
1. **Funnel Analysis:** Conversion paths
2. **Cohort Analysis:** User groups over time
3. **Path Analysis:** User flow patterns
4. **Segmentation:** User group differences
5. **Correlation Analysis:** Relationship discovery

**Best Practices:**
1. Define clear questions upfront
2. Verify data quality
3. Understand data limitations
4. Combine with qualitative for "why"
5. Look for anomalies and patterns
6. Create actionable segments
7. Validate findings with other methods

**Deliverables:**
- Behavioral insights report
- Conversion funnel analysis
- User segment profiles
- Opportunity identification
- Baseline metrics

---

## Mixed Methods

### Sequential Explanatory Design

**Structure:** Quantitative → Qualitative

**Process:**
1. Collect and analyze quantitative data
2. Identify patterns needing explanation
3. Design qualitative follow-up
4. Collect qualitative data
5. Use qual to explain quant findings

**When to Use:**
- Explaining survey results
- Understanding statistical outliers
- Adding context to metrics
- Investigating unexpected findings

**Best Practices:**
- Design phases to complement
- Sample from quant participants for qual
- Focus qual on explanation needs
- Integrate findings in final analysis

---

### Sequential Exploratory Design

**Structure:** Qualitative → Quantitative

**Process:**
1. Conduct qualitative exploration
2. Identify themes and hypotheses
3. Design quantitative validation
4. Test at scale with quant methods
5. Validate and generalize findings

**When to Use:**
- Developing new measures
- Testing emergent theories
- Scaling exploratory findings
- Building on qualitative insights

**Best Practices:**
- Use qual to inform quant design
- Ensure sufficient qual depth first
- Operationalize qual themes carefully
- Link findings explicitly

---

### Concurrent Triangulation Design

**Structure:** Qualitative + Quantitative (Parallel)

**Process:**
1. Design complementary qual and quant
2. Collect both data types simultaneously
3. Analyze separately
4. Compare and contrast findings
5. Integrate for comprehensive understanding

**When to Use:**
- Comprehensive understanding needed
- Validation through triangulation
- Time constraints prevent sequential
- Different stakeholder needs

**Best Practices:**
- Ensure equal priority to both
- Plan integration strategy upfront
- Look for convergence and divergence
- Use mixing matrix for integration

---

## Method Selection Framework

### Decision Criteria Matrix

| Factor | Favors Qualitative | Favors Quantitative | Favors Mixed |
|--------|-------------------|---------------------|--------------|
| Research Stage | Early, exploratory | Later, validation | Multiple stages |
| Question Type | How, why | How many, how much | Both types |
| Required Depth | Deep understanding | Surface patterns | Comprehensive |
| Sample Size | Small, purposive | Large, representative | Both needed |
| Generalizability | Not required | Critical | Context-dependent |
| Timeline | Flexible | Fixed, longer | Extended |
| Resources | Moderate | Higher for scale | Highest |

### Research Question Mapping

**Exploratory Questions → Qualitative**
- What are users experiencing?
- How do users accomplish tasks?
- Why do users behave this way?
- What are unmet needs?

**Descriptive Questions → Quantitative**
- How many users experience this?
- What is the frequency?
- Which segments exist?
- What correlates with behavior?

**Explanatory Questions → Mixed Methods**
- Why does this pattern exist?
- How can we explain these numbers?
- What drives these differences?
- How do factors interact?

---

## Sample Size Guidelines

### Qualitative Sample Sizes

| Method | Minimum | Typical | Maximum | Saturation Point |
|--------|---------|---------|---------|------------------|
| IDIs | 12 | 15-20 | 30 | 12-15 interviews |
| Focus Groups | 3 groups | 4-5 groups | 8 groups | 3-4 groups |
| Ethnography | 6 | 8-12 | 20 | 8-10 observations |
| Diary Studies | 10 | 12-15 | 25 | 12-15 participants |
| Card Sorting | 15 | 20-25 | 40 | 15-20 participants |

### Quantitative Sample Sizes

**Survey Sample Size Formula:**
n = (Z² × p × (1-p)) / e²

Where:
- Z = Z-score (1.96 for 95% confidence)
- p = Expected proportion (0.5 if unknown)
- e = Margin of error (0.05 for ±5%)

**Common Sample Sizes:**
- Population > 10,000: 370-385 (95% confidence, ±5% margin)
- Population 1,000: 278
- Population 500: 217
- Population 100: 80

**A/B Test Sample Size:**
Use statistical power calculators with:
- Power: 80% minimum
- Significance: 5%
- Minimum detectable effect
- Baseline conversion rate

---

## Timeline Estimations

### Typical Research Timeline by Method

| Method | Planning | Recruitment | Data Collection | Analysis | Reporting | Total |
|--------|----------|-------------|-----------------|----------|-----------|--------|
| IDIs (20) | 1 week | 1-2 weeks | 2-3 weeks | 1-2 weeks | 1 week | 6-9 weeks |
| Focus Groups (4) | 1 week | 2-3 weeks | 1 week | 1 week | 1 week | 6-7 weeks |
| Survey (n=500) | 1 week | 1 week | 1-2 weeks | 1 week | 1 week | 5-6 weeks |
| Ethnography (10) | 1 week | 1-2 weeks | 2-3 weeks | 2 weeks | 1 week | 7-9 weeks |
| Diary Study | 1 week | 1 week | 2-4 weeks | 1-2 weeks | 1 week | 6-9 weeks |
| Conjoint Analysis | 2 weeks | 1 week | 1-2 weeks | 1 week | 1 week | 6-7 weeks |
| A/B Test | 1 week | N/A | 2-4 weeks | 1 week | 3 days | 4-6 weeks |

### Rapid Research Options

**5-Day Sprint:**
- Day 1: Planning and recruitment start
- Day 2-3: Data collection
- Day 4: Analysis
- Day 5: Reporting

**2-Week Discovery:**
- Week 1: Planning, recruitment, initial interviews
- Week 2: Additional data collection, analysis, report

---

## Cost Considerations

### Typical Cost Ranges (USD)

**Direct Costs:**
- Participant incentives: $50-200 per hour
- Recruitment fees: $50-150 per participant
- Tool licenses: $100-1000 per month
- Transcription: $1-2 per minute
- Travel (if needed): Variable

**Method-Specific Budgets:**

| Method | Low Budget | Medium Budget | High Budget |
|--------|------------|---------------|-------------|
| IDIs (20) | $3,000 | $8,000 | $15,000+ |
| Focus Groups (4) | $8,000 | $15,000 | $25,000+ |
| Survey (n=500) | $2,500 | $7,500 | $15,000+ |
| Ethnography (10) | $5,000 | $12,000 | $25,000+ |
| Conjoint | $5,000 | $15,000 | $30,000+ |
| A/B Test | $1,000 | $5,000 | $10,000+ |

---

## Quality Standards

### Research Quality Checklist

**Planning Phase:**
- [ ] Clear research questions defined
- [ ] Appropriate method selected
- [ ] Sample criteria established
- [ ] Ethics approval obtained (if needed)
- [ ] Pilot testing planned

**Execution Phase:**
- [ ] Screening criteria applied consistently
- [ ] Data collection protocols followed
- [ ] Consent obtained and documented
- [ ] Data quality checks performed
- [ ] Backup systems in place

**Analysis Phase:**
- [ ] Analysis framework defined
- [ ] Multiple analyst validation (if applicable)
- [ ] Negative cases examined
- [ ] Bias mitigation applied
- [ ] Findings traceable to data

**Reporting Phase:**
- [ ] Limitations acknowledged
- [ ] Methodology transparent
- [ ] Findings actionable
- [ ] Recommendations evidence-based
- [ ] Stakeholder feedback incorporated

### Validity and Reliability

**Ensuring Validity:**
- Triangulation across methods
- Member checking with participants
- Peer debriefing
- Thick description in reporting
- Audit trail maintenance

**Ensuring Reliability:**
- Inter-rater reliability for coding
- Test-retest for surveys
- Internal consistency checks
- Protocol standardization
- Training for consistency

---

## Quick Reference Decision Tree

```
Start: What do you need to know?
│
├─ WHY something happens → Qualitative
│  ├─ Individual experiences → IDIs
│  ├─ Group dynamics → Focus Groups
│  └─ Context critical → Ethnography
│
├─ HOW MANY/MUCH → Quantitative
│  ├─ Attitudes/preferences → Survey
│  ├─ Trade-offs → Conjoint
│  └─ Causal impact → A/B Test
│
└─ BOTH WHY and HOW MANY → Mixed Methods
   ├─ Explain patterns → Sequential Explanatory
   ├─ Scale insights → Sequential Exploratory
   └─ Comprehensive → Concurrent Triangulation
```

---

## References and Further Reading

- Creswell, J. W. (2021). *Research Design: Qualitative, Quantitative, and Mixed Methods*
- Krueger, R. A. (2014). *Focus Groups: A Practical Guide*
- Kvale, S. (2007). *Doing Interviews*
- Lazar, J. (2017). *Research Methods in Human-Computer Interaction*
- Orme, B. (2019). *Getting Started with Conjoint Analysis*
- Patton, M. Q. (2014). *Qualitative Research & Evaluation Methods*
- Saldaña, J. (2021). *The Coding Manual for Qualitative Researchers*